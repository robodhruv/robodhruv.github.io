<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-77776666-4"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-77776666-4');
  </script>

  <!-- Scramble Script by Jeff Donahue -->
  <script src="./scramble.js"></script>
  <title>Dhruv Shah</title>

  <meta name="author" content="Dhruv Shah">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="assets/favicon.png">
</head>

<body>
  <table
    style="width:100%;max-width:1020px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tbody>
      <tr>
        <td>

          <p align="center">
            <name>Dhruv Shah</name><br>
            <email>
              <font id="email" style="display:inline;">rsvha.uedeeh@.dyrulkbeh</font>
              <script>
                emailScramble = new scrambledString(document.getElementById('email'),
                  'emailScramble', 'es.edlra@ecyheb.kueesh',
                  [12, 1, 10, 6, 21, 16, 13, 3, 5, 20, 8, 18, 4, 17, 11, 19, 14, 22, 15, 7, 9, 2]);
              </script>
            </email>
          </p>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">


            <tbody>
              <tr>
                <td width="70%" valign="middle" align="justify">
                  <wall>
                    <p> I am a Senior Research Scientist at <a href="https://deepmind.google">Google DeepMind</a>,
                      working foundation models of and for robotics.
                      Previously, I obtained my PhD in EECS at UC Berkeley, where I was advised by <a
                        href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>. My
                      research was supported by the Berkeley Fellowship for Graduate Study, and has been nominated for
                      (and received) several Best Paper Awards at leading robotics conferences, including RSS and ICRA.
                    </p>

                    <p> Earlier, I graduated with honors from IIT Bombay, where I received the Undergraduate Research
                      Award and the Institute Academic Prize. I have also been fortunate to spend time at Meta AI
                      (FAIR),
                      Google DeepMind (Brain Robotics), Carnegie Mellon University, Imperial College London and the
                      University of Sydney.
                    </p>

                    <p> I will join Princeton next academic year as an Assistant Professor in ECE and Robotics! If you are interested in working with me, please apply to the <a href="https://gradschool.princeton.edu/admission-onboarding/apply">centralized admission portal</a> and mention my name.</p>

                    <p align="center">
                      <a href="./docs/cv.pdf">CV</a> / <a
                        href="https://scholar.google.com/citations?hl=en&user=d5y4iKAAAAAJ&view_op=list_works">Scholar</a>
                      / <a href="https://twitter.com/shahdhruv_"> Twitter </a> /
                      <a href="https://www.linkedin.com/in/dhruv-ilesh-shah/"> LinkedIn </a>
                    </p>


                  </wall>
                </td>

                <td width="25%" valign="top"><a href="./assets/DhruvShah24.jpeg"><img src="./assets/DhruvShah24.jpeg"
                      width="100%" style="border-radius: 10%"></a></td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Updates</heading>
                  <p>
                  <ul id="news">
                    <li> <i>[Aug 2024]</i>&nbsp; I defended my PhD thesis at Berkeley!
                    <li> <i>[July 2024]</i>&nbsp; I started full-time at Google DeepMind Robotics
                    <li> <i>[May 2024]</i>&nbsp; <a
                        href="https://general-navigation-models.github.io/nomad/index.html">NoMaD</a> and <a
                        href="https://robotics-transformer-x.github.io">RT-X</a> both won the Best Conference Paper
                      award at ICRA 2024!
                    <li> <i>[Mar 2024]</i>&nbsp; I was named a <a href="https://robotics.umd.edu/futureleaders">2024
                        Microsoft Future Leader in Robotics and AI</a> by the University of Maryland
                    <li> <i>[Feb 2024]</i>&nbsp; I will be giving invited talks at MIT, Princeton, UCSD, UCLA, Michigan,
                      Purdue, and Columbia
                    <li> <i>[Jan 2024]</i>&nbsp; <a
                        href="https://general-navigation-models.github.io/nomad/index.html">NoMaD</a> and <a
                        href="https://robotics-transformer-x.github.io">RT-X</a> were accepted to ICRA 2024 with 6 best
                      paper award
                      nominations!
                    <li> <i>[Dec 2023]</i>&nbsp; I will be speaking at the <a href="https://ml4ad.github.io">ML4AD
                        workshop</a> at NeurIPS 2023
                    <li> <i>[Nov 2023]</i>&nbsp; I am on the job market! Please reach out if you think I would be a good
                      fit for your team/department.
                    <li> <i>[Oct 2023]</i>&nbsp; Our work on <a href="https://grounded-decoding.github.io">grounding
                        language models</a> for robotic tasks was accepted to NeurIPS 2023!
                    <li> <i>[Oct 2023]</i>&nbsp; Our work on <a href="https://sites.google.com/view/SACSoN-review">data
                        collection for social navigation</a> was
                      accepted to RA Letters and will be presented at ICRA 2024!
                    <li> <i> [Sep 2023]</i>&nbsp; We are organizing the <a
                        href="https://sites.google.com/view/langrob-corl23/home">2nd Workshop on Language and Robot
                        Learning</a> @ CoRL 2023: consider submitting </li>
                    <li> <i> [Sep 2023]</i>&nbsp; Three papers to appear at CoRL 2023: <a
                        href="https://visualnav-transformer.github.io">a foundation model for visual navigation</a>,
                      <a href="https://sites.google.com/view/fastrlap">autonomous high-speed off-road driving</a>, and
                      <a>coming soon</a>!
                    </li>
                    <li> <i> [Aug 2023]</i>&nbsp; We are organizing the <a href="http://www.robot-learning.ml/2023/">6th
                        Robot Learning Workshop</a> @ NeurIPS 2023: consider submitting </li>
                    <li> <i> [Jan 2023]</i>&nbsp; Two papers on generalizable navigation
                      policies across robots to accepted to ICRA 2023: <a
                        href="https://sites.google.com/view/drive-any-robot">GNM</a> and <a
                        href="https://sites.google.com/view/exaug-nav">Ex-Aug</a>!
                    </li>
                    <li> <i> [Jan 2023]</i>&nbsp; We are organizing the 2nd Workshop on Learning from Diverse,
                      Offline Data at ICRA 2023 &mdash; consider submitting </li>
                    <li> <i> [Dec 2022]</i>&nbsp; We published an opinion and survey paper on <a
                        href="https://royalsocietypublishing.org/doi/full/10.1098/rstb.2021.0447">data-driven
                        navigation</a>!</li>
                    <li> <i> [Oct 2022]</i>&nbsp; <a href="https://sites.google.com/view/lmnav">LM-Nav</a>
                      was featured in <a href="https://youtu.be/xYvJV_z5Sxc">a video by Two Minute Papers</a>!</li>
                    <li> <i> [Sep 2022]</i>&nbsp; We are organizing the <a
                        href="https://sites.google.com/view/langrob-corl22/home">Workshop on Language and Robot
                        Learning</a> at CoRL 2022 &mdash; consider submitting</li>
                    <li> <i> [Sep 2022]</i>&nbsp; Two papers accepted to CoRL 2022: <a
                        href="https://sites.google.com/view/lmnav">LM-Nav</a> and <a
                        href="https://sites.google.com/view/revind">ReViND</a>!</li>
                    <li> <i> [Apr 2022]</i>&nbsp; Our paper on <a
                        href="https://sites.google.com/view/viking-release/home">kilometer-scale visual navigation</a>
                      was accepted to RSS 2022 as a <b>Best Systems Paper Finalist</b></li>
                    <li> <i> [Mar 2022]</i>&nbsp; We are organizing the <a
                        href="https://sites.google.com/view/l-dod-rss2022/home">Workshop on Learning from Diverse,
                        Offline Data</a> at RSS 2022 &mdash; consider submitting</li>
                    <li> <i> [Mar 2022]</i>&nbsp; <a href="https://sites.google.com/view/viking-release/home">ViKiNG</a>
                      was featured in the news &mdash; see coverage by <a
                        href="https://spectrum.ieee.org/uc-berkeley-viking-navigation">IEEE Spectrum</a> and <a
                        href="https://www.zdnet.com/article/uc-berkeley-robot-navigation-could-chart-a-new-course-for-self-driving-systems/">ZDNet</a>!
                    </li>
                    <!--<li> <i> [Feb 2022]</i>&nbsp; New work on <a href="https://sites.google.com/view/viking-release/home">kilometer-scale visual navigation</a> released on arXiv!</li>-->
                    <li> <i> [Jan 2022]</i>&nbsp; Our paper on <a href="https://arxiv.org/abs/2111.03189">skill-centric
                        state abstractions</a> was accepted to ICLR 2022</li>
                    <li> <i> [Jan 2022]</i>&nbsp; Our paper on <a href="https://arxiv.org/abs/2111.10948">combining
                        geometric costmaps with learned models</a> was accepted to ICRA 2022</li>
                    <!-- <li> <i> [Nov 2021]</i>&nbsp; I'll be presenting <a href="https://sites.google.com/view/recon-robot/">RECON</a> in beautiful London at CoRL 2021, please stop by if you're attending</li> -->
                    <li> <i> [Oct 2021]</i>&nbsp; Check out this <a
                        href="https://medium.com/@sergey.levine/how-robots-can-learn-end-to-end-from-data-3d879b0a2ba1">blog
                        post by Sergey on data-driven robot learning</a>, featuring some of my recent research</li>
                    <li> <i> [Sep 2021]</i>&nbsp; Our paper on <a
                        href="https://sites.google.com/view/recon-robot/">goal-directed exploration on mobile
                        robots</a>
                      was accepted as an <b>Oral Presentation</b> to CoRL 2021</li>
                    <li> <i> [Aug 2021]</i>&nbsp; I'm teaching the <a
                        href="https://rail.eecs.berkeley.edu/deeprlcourse/">Berkeley Deep RL class (CS285)</a> as Head
                      GSI this semester</li>
                    <li> <i> [May 2021]</i>&nbsp; I'll be spending the summer at Google Brain Robotics, interning with
                      Brian Ichter and Alex Toshev</li>
                    <!--<li> <i> [Apr 2021]</i>&nbsp; <a href="https://arxiv.org/abs/2104.05859">New pre-print</a> on deploying robots in novel, unstructured environments for goal-directed exploration</li>-->
                    <li> <i> [Feb 2021]</i>&nbsp; Our paper on <a
                        href="https://sites.google.com/view/ving-robot/">navigating to visual goals in the
                        real-world</a> was accepted to ICRA 2021</li>
                    <li> <i> [May 2020]</i>&nbsp; Our paper on <a
                        href="http://www.roboticsproceedings.org/rss16/p046.html">hybrid control for aerial
                        manipulation</a> was accepted to RSS 2020</li>
                    <li> <i> [Jan 2020]</i>&nbsp; Our paper on <a
                        href="https://openreview.net/forum?id=rJe2syrtvS">analyzing ingredients of real-world RL for
                        robots</a> was accepted to ICLR 2020</li>
                    <!-- <li> <i> [Aug 2019]</i>&nbsp; I graduated from IIT Bombay, receiving the Undergraduate Research Award </li>
                <li> <i> [May 2019]</i>&nbsp; I'll be interning at the <a href="https://sydney.edu.au/engineering/our-research/robotics-and-intelligent-systems/australian-centre-for-field-robotics.html">Australian Centre for Field Robotics</a> over Summer 2019</li>
                <li> <i> [Apr 2019]</i>&nbsp; Our paper on projection design for source separation accepted to ICIP 2019 </li>
                <li> <i> [Apr 2019]</i>&nbsp; I'll be joining UC Berkeley at <a href="https://bair.berkeley.edu/">BAIR</a> as a PhD student in Fall 2019
                <li> <i> [Mar 2019]</i>&nbsp; Awarded the <a href="https://inae.in/inae-travel-grant/">INAE Travel Grant</a> to present at ICRA 2019, Montréal </li>
                <li> <i> [Jan 2019]</i>&nbsp; Our paper on<a href="https://ieeexplore.ieee.org/document/8613876">swarm aggregation without communication</a> will appear in RA-L and ICRA 2019 </li>
                <li> <i> [Nov 2018]</i>&nbsp; Awarded the <a href="https://inae.in/inae-travel-grant/">INAE Travel Grant</a> to present at GlobalSIP 2018, Anaheim </li>
                <li> <i> [Sep 2018]</i>&nbsp; Our paper on <a href="https://cs.berkeley.edu/~shah/constrained-projections">designing constrained projections for CS</a> was accepted to GlobalSIP 2018 </li>
                <li> <i> [Aug 2018]</i>&nbsp; Honored to receive the Institute Academic Prize 2017-18 </li>
                <li> <i> [May 2018]</i>&nbsp; I'll be interning at <a href="http://www.imperial.ac.uk/dyson-robotics-lab/">Dyson Robotics Lab</a>, Imperial College London over Summer 2018 </li>
                <li> <i> [Dec 2017]</i>&nbsp; Our paper on <a href="http://home.iitb.ac.in//~dhruv.shah/research/2018/03/30/radloc-wm2018.html">radiation localization</a> was accepted to WM2018 </li> -->
                  </ul>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Blog Posts</heading>
                  <p>
                  <ul id="blogs">
                    <li> <a
                        href="https://deepmind.google/discover/blog/scaling-up-learning-across-many-different-robot-types/">Scaling
                        up learning across many different robot types</a>: massive inter-university collaboration on
                      cross-embodiment learning </li>
                    <li> <a href="https://ai.googleblog.com/2022/04/extracting-skill-centric-state.html">Extracting
                        Skill-Centric State Abstractions from Value Functions</a>: our work on obtaining state
                      abstractions that are informed by capabilities of low-level skills </li>
                    <li> <a href="https://bair.berkeley.edu/blog/2021/11/03/recon/">Learning to Explore the Real World
                        with a Ground Robot</a>: our work on goal-directed exploration with a mobile robot</li>
                    <li> <a href="https://bair.berkeley.edu/blog/2020/04/27/ingredients/">The Ingredients of Real World
                        Robotic Reinforcement Learning</a>: our work on robot learning in the real-world without
                      instrumentation</li>
                    <!-- https://medium.com/@sergey.levine/how-robots-can-learn-end-to-end-from-data-3d879b0a2ba1 -->
                  </ul>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Pre-Prints</heading>
                  <p>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              
            </tbody>
          </table> -->

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Publications</heading>
                  <p>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='assets/rtx_arxiv23.gif' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://robotics-transformer-x.github.io">
                    <papertitle>Open X-Embodiment: Robotic Learning Datasets and RT-X Models
                    </papertitle>
                  </a>
                  <br> <br>
                  Open X-Embodiment Collaboration
                  <p> <b style="color:#FDB515">Best Conference Paper Award</b> <br>
                    <b style="color:#FDB515">Best Student Paper Award (Finalist)</b> <br>
                    <b style="color:#FDB515">Best Paper Award in Robot Manipulation (Finalist)</b> <br>
                    <i> International Conference on Robotics and Automation (ICRA), 2024 </i>
                    <br><i>CoRL 2023 Workshop Towards Generalist Robots</i> &nbsp; <b style="color:#FDB515">(Oral
                      Presentation)</b>
                  </p>
                  <p style="font-size: 13px"> <a href="https://arxiv.org/abs/2310.08864">arXiv</a> / <a
                      href="https://deepmind.google/discover/blog/scaling-up-learning-across-many-different-robot-types/">Blog
                      Post</a> /
                    <a
                      href="https://docs.google.com/spreadsheets/d/1rPBD77tk60AEIGZrGSODwyyzs5FgCU9Uz3h-3_t2A9g/edit#gid=0/">Dataset</a>
                    /
                    <a href="https://github.com/google-deepmind/open_x_embodiment">Code</a>
                  </p>
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='assets/nomad_icra24.gif' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://general-navigation-models.github.io/nomad">
                    <papertitle>NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration
                    </papertitle>
                  </a>
                  <br> <br>
                  Ajay Sridhar,
                  <strong>Dhruv Shah</strong>,
                  Catherine Glossop,
                  Sergey Levine
                  <p> <b style="color:#FDB515">Best Conference Paper Award</b> <br>
                    <b style="color:#FDB515">Best Student Paper Award (Finalist)</b> <br>
                    <b style="color:#FDB515">Best Paper Award in Cognitive Robotics (Finalist)</b> <br>
                    <i> International Conference on Robotics and Automation (ICRA), 2024 </i>
                    <br><i>CoRL 2023 Workshop on Pre-Training for Robot Learning</i> &nbsp; <b
                      style="color:#FDB515">(Oral Presentation)</b>
                    <br><i>NeurIPS 2023 Workshop on Foundation Models for Decision-Making</i> &nbsp; <b
                      style="color:#FDB515">(Oral Presentation)</b>
                  </p>
                  <p style="font-size: 13px"> <a href="https://arxiv.org/abs/2306.01874">arXiv</a> / <a
                      href="https://www.youtube.com/watch?v=zH8LaIapF6w">Summary Video</a> /
                    <a href="https://general-navigation-models.github.io/nomad/">Dataset</a> /
                    <a href="https://github.com/PrieureDeSion/visualnav-transformer">Code</a>
                  </p>
                  </p>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='assets/sacson_arxiv23.gif' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://sites.google.com/view/SACSoN-review">
                    <papertitle>SACSoN: Scalable Autonomous Data Collection for Social Navigation
                    </papertitle>
                  </a>
                  <br> <br>
                  Noriaki Hirose,
                  <strong>Dhruv Shah</strong>,
                  Ajay Sridhar,
                  Sergey Levine
                  <p> <i> Robotics and Automation Letters (RA-L), 2023 </i>
                    <br> <i>International Conference on Robotics and Automation (ICRA), 2024</i>
                    <br><i> Conference on Robot Learning (CoRL), 2023 </i> &nbsp; <b style="color:#FDB515">(Live
                      Demo)</b>
                  </p>
                  <p style="font-size: 13px"> <a href="https://arxiv.org/abs/2306.01874">arXiv</a> / <a
                      href="https://www.youtube.com/watch?v=AuYwmlUzi28">Summary Video</a> /
                    <a href="https://sites.google.com/view/sacson-review/sacson-dataset">Dataset</a>
                  </p>
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='assets/gd_arxiv2023.gif' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://grounded-decoding.github.io">
                    <papertitle>Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control
                    </papertitle>
                  </a>
                  <br> <br>
                  Wenlong Huang,
                  Fei Xia,
                  <strong>Dhruv Shah</strong>,
                  Danny Driess,
                  Andy Zeng,
                  Yao Lu,
                  Pete Florence,
                  Igor Mordatch,
                  Sergey Levine,
                  Karol Hausman,
                  Brian Ichter
                  <p> <i> Advances in Neural Information Processing Systems (NeurIPS) 2023 </i></p>
                  <p style="font-size: 13px"> <a href="https://arxiv.org/abs/2303.00855">arXiv</a> / <a
                      href="https://www.youtube.com/watch?v=KHhAlBIQftQ">Summary Video</a> </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='assets/vint_arxiv2023.gif' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://visualnav-transformer.github.io">
                    <papertitle>ViNT: A Foundation Model for Visual Navigation
                    </papertitle>
                  </a>
                  <br> <br>
                  <strong>Dhruv Shah</strong><sup>†</sup>,
                  Ajay Sridhar<sup>†</sup>,
                  Nitish Dashora<sup>†</sup>,
                  Kyle Stachowicz,
                  Kevin Black,
                  Noriaki Hirose,
                  Sergey Levine
                  <p> <i> Conference on Robot Learning (CoRL), 2023 </i> &nbsp; <b style="color:#FDB515">(Oral
                      Presentation & Live Demo)</b>
                    <br> <i> Bay Area Machine Learning Symposium (BayLearn) 2022 </i> &nbsp; <b
                      style="color:#FDB515">(Oral Presentation)</b>
                  <p style="font-size: 13px"> <a href="https://arxiv.org/abs/2306.14846">arXiv</a> / <a
                      href="https://www.youtube.com/watch?v=6kNex5dJ5sQ">Summary Video</a> /
                    <a href="https://github.com/PrieureDeSion/visualnav-transformer">Code</a>
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='assets/fastrlap_arxiv23.gif' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://sites.google.com/view/fastrlap">
                    <papertitle>FastRLAP: A System for Learning High-Speed Driving via Deep RL and Autonomous Practicing
                    </papertitle>
                  </a>
                  <br> <br>
                  Kyle Stachowicz<sup>†</sup>,
                  <strong>Dhruv Shah</strong><sup>†</sup>,
                  Arjun Bhorkar,
                  Ilya Kostrikov,
                  Sergey Levine
                  <p> <i> Conference on Robot Learning (CoRL), 2023 </i>
                  <p style="font-size: 13px"> <a href="https://arxiv.org/abs/2304.09831">arXiv</a> / <a
                      href="https://youtu.be/eZATlf0ybXk">Summary Video</a> /
                    <a href="https://github.com/kylestach/fastrlap-release">Code</a> /
                    <a href="https://techxplore.com/news/2023-05-robotic-cars.html">Media
                      Coverage</a> <sup><a href="
                        https://techxplore.com/news/2023-05-robotic-cars.html">1</a>,
                      <a
                        href="
                      https://syncedreview.com/2023/04/27/uc-berkeleys-fastrlap-learns-aggressive-and-effective-high-speed-driving-strategies-with/">2</a>,
                      <a
                        href="https://www.marktechpost.com/2023/05/02/uc-berkeley-researchers-propose-fastrlap-a-system-for-learning-high-speed-driving-via-deep-rl-reinforcement-learning-and-autonomous-practicing/">3</a>,
                      <a href="https://www.techeblog.com/fastrlap-robot-cars-drive-fast-autonomously/">4</a></sup>
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='assets/lfg_arxiv23.gif' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://sites.google.com/view/lfg-nav/">
                    <papertitle>Navigation with Large Language Models: Semantic Guesswork as a Heuristic for Planning
                    </papertitle>
                  </a>
                  <br> <br>
                  <strong>Dhruv Shah</strong><sup>†</sup>,
                  Michael Equi<sup>†</sup>,
                  Blazej Osinski,
                  Fei Xia,
                  Brian Ichter,
                  Sergey Levine
                  <p> <i> Conference on Robot Learning (CoRL), 2023 </i>
                  <p style="font-size: 13px"> <a href="https://arxiv.org/abs/2310.10103">arXiv</a> / <a
                      href="https://www.youtube.com/watch?v=jUqYQOW7E-4">Summary Video</a> /
                    <a href="https://github.com/Michael-Equi/lfg-nav">Code</a> /
                    <a
                      href="https://colab.research.google.com/github/Michael-Equi/lfg-demo/blob/main/lfg_demo_colab.ipynb">Interactive
                      Colab</a>
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='assets/exaug_icra23.gif' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://sites.google.com/view/exaug-nav">
                    <papertitle>ExAug: Robot-Conditioned Navigation Policies via Geometric Experience Augmentation
                    </papertitle>
                  </a>
                  <br> <br>
                  Noriaki Hirose,
                  <strong>Dhruv Shah</strong>,
                  Ajay Sridhar,
                  Sergey Levine
                  <p> <i> International Conference on Robotics and Automation (ICRA), 2023 </i></p>
                  <p style="font-size: 13px"> <a href="https://arxiv.org/abs/2210.07450">arXiv</a> / <a
                      href="https://www.youtube.com/watch?v=zWGpaxa1DKY">Summary Video</a> </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='assets/gnm_icra23.gif' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://sites.google.com/view/drive-any-robot">
                    <papertitle>GNM: A General Navigation Model to Drive Any Robot
                    </papertitle>
                  </a>
                  <br> <br>
                  <strong>Dhruv Shah</strong><sup>†</sup>,
                  Ajay Sridhar<sup>†</sup>,
                  Arjun Bhorkar,
                  Noriaki Hirose,
                  Sergey Levine
                  <p> <i> International Conference on Robotics and Automation (ICRA), 2023 </i></p>
                  <p style="font-size: 13px"> <a href="https://arxiv.org/abs/2210.03370">arXiv</a> / <a
                      href="https://youtu.be/ICeD6iOglKc">Summary Video</a> /
                    <a href="https://github.com/PrieureDeSion/drive-any-robot">Code</a> /
                    <a
                      href="https://www.marktechpost.com/2022/12/22/this-artificial-intelligence-ai-paper-from-uc-berkeley-presents-a-general-navigation-model-gnm-from-an-aggregated-multirobot-dataset-to-drive-any-robot/">Media
                      Coverage</a>
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='assets/experiential_rstb22.gif' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://royalsocietypublishing.org/doi/full/10.1098/rstb.2021.0447">
                    <papertitle>Learning Robotic Navigation from Experience: Principles, Methods, and Recent Results
                    </papertitle>
                  </a>
                  <br> <br>
                  Sergey Levine,
                  <strong>Dhruv Shah</strong>
                  <p>
                    <i> Philosophical Transactions of the Royal Society B, 2022 </i> &nbsp; <b
                      style="color:#FDB515">(Invited
                      Paper)</b>
                  </p>
                  <p style="font-size: 13px"> <a href="https://arxiv.org/abs/2212.06759">arXiv</a></p>
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='assets/revind_corl22.png' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://sites.google.com/view/revind">
                    <papertitle>Offline Reinforcement Learning for Visual Navigation
                    </papertitle>
                  </a>
                  <br> <br>
                  <strong>Dhruv Shah</strong><sup>†</sup>,
                  Arjun Bhorkar<sup>†</sup>,
                  Hrish Leen,
                  Ilya Kostrikov,
                  Nick Rhinehart,
                  Sergey Levine
                  <p> <i> Conference on Robot Learning (CoRL), 2022 </i> &nbsp; <b style="color:#FDB515">(Oral
                      Presentation)</b>
                  </p>

                  <p style="font-size: 13px"> <a href="https://arxiv.org/abs/2212.08244">arXiv</a> /
                    <a href="https://youtu.be/nMS8IOqZRwo?t=164">Talk @ CoRL</a> /
                    <a href="https://github.com/arjunbhorkar/ReViND">Code</a>
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='assets/lmnav_corl22.gif' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://sites.google.com/view/lmnav">
                    <papertitle>LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action
                    </papertitle>
                  </a>
                  <br> <br>
                  <strong>Dhruv Shah</strong><sup>†</sup>,
                  Blazej Osinski<sup>†</sup>,
                  Brian Ichter,
                  Sergey Levine
                  <p> <i> Conference on Robot Learning (CoRL), 2022 </i>
                    <br> <i> Foundation Models for Decision Making Workshop at NeurIPS 2022 </i> &nbsp; <b
                      style="color:#FDB515">(Oral Presentation)</b>
                    <br> <i> Bay Area Machine Learning Symposium (BayLearn) 2022 </i> &nbsp; <b
                      style="color:#FDB515">(Oral Presentation)</b>
                  </p>

                  <p style="font-size: 13px"> <a href="https://arxiv.org/abs/2207.04429">arXiv</a> / <a
                      href="https://www.youtube.com/watch?v=wkVbuZQb_5g">Summary Video</a> /
                    <a href="https://github.com/blazejosinski/lm_nav">Code</a> /
                    <a
                      href="https://colab.research.google.com/github/blazejosinski/lm_nav/blob/main/colab_experiment.ipynb">Interactive
                      Colab</a> /
                    <a href="https://www.youtube.com/watch?v=xYvJV_z5Sxc">2MP Feature</a> /
                    <a href="https://youtu.be/Bd4FV3B7du4?t=383">Spotlight @ CoRL</a> /
                    <a
                      href="https://www.utmel.com/blog/news/other/google-unveils-lm-nav-a-robotic-navigation-system-in-association-with-universities">Media
                      Coverage</a>
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='assets/viking_rss22.gif' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="http://sites.google.com/view/viking-release/">
                    <papertitle>ViKiNG: Vision-Based Kilometer-Scale Navigation with Geographic Hints</papertitle>
                  </a>
                  <br> <br>
                  <strong>Dhruv Shah</strong>,
                  Sergey Levine
                  <p> <b style="color:#FDB515">Best Systems Paper Finalist</b> <br>
                    <i> Robotics: Science and Systems (RSS), 2022 </i> &nbsp; <b style="color:#FDB515">(Oral
                      Presentation)</b>
                  </p>
                  <p style="font-size: 13px"><a href="https://arxiv.org/abs/2202.11271">arXiv</a> /
                    <a href="https://www.youtube.com/watch?v=rON5ew8hW60">Summary Video</a> / <a
                      href="https://youtu.be/qI0zvRp-UnE?t=18893">Talk @ RSS</a> /
                    <a href="https://spectrum.ieee.org/uc-berkeley-viking-navigation">Media Coverage</a> <sup>
                      <a href="https://spectrum.ieee.org/uc-berkeley-viking-navigation">1</a>,
                      <a
                        href="https://www.zdnet.com/article/uc-berkeley-robot-navigation-could-chart-a-new-course-for-self-driving-systems/">2</a>,
                      <a
                        href="https://www.wevolver.com/article/viking-the-hiking-robot-can-plan-its-journey-using-overhead-maps-even-if-theyre-inaccurate">3</a></sup>
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='assets/hip_icra22.gif' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://sites.google.com/view/hybrid-imitative-planning">
                    <papertitle>Hybrid Imitative Planning with Geometric and Predictive Costs in Off-road Environments
                    </papertitle>
                  </a>
                  <br> <br>
                  Nitish Dashora<sup>†</sup>,
                  Daniel Shin<sup>†</sup>,
                  <strong>Dhruv Shah</strong>,
                  Henry Leopold,
                  David Fan,
                  Ali Agha-Mohammadi,
                  Nicholas Rhinehart,
                  Sergey Levine
                  <p> <i> International Conference on Robotics and Automation (ICRA), 2022 </i></p>
                  <p style="font-size: 13px"> <a href="https://arxiv.org/abs/2111.10948">arXiv</a> / <a
                      href="https://www.youtube.com/watch?v=oHUrW-m-L1Y">Talk @ ICRA</a> </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='assets/vfs_iclr22.gif' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="./VFS">
                    <papertitle>Value Function Spaces: Skill-Centric State Abstractions for Long-Horizon Reasoning
                    </papertitle>
                  </a>
                  <br> <br>
                  <strong>Dhruv Shah</strong>,
                  Peng Xu,
                  Yao Lu,
                  Ted Xiao,
                  Alexander Toshev,
                  Sergey Levine,
                  Brian Ichter
                  <p> <i> International Conference on Learning Representations (ICLR), 2022 </i></p>
                  <p style="font-size: 13px"> <a
                      href="https://ai.googleblog.com/2022/04/extracting-skill-centric-state.html"> Blog Post </a> / <a
                      href="https://arxiv.org/abs/2111.03189">arXiv</a> / <a href="https://youtu.be/KwBMWcPhZZQ">Talk @
                      ICLR</a> </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='assets/recon_corl21.gif' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="http://sites.google.com/view/recon-robot/">
                    <papertitle>Rapid Exploration for Open-World Navigation with Latent Goal Models</papertitle>
                  </a>
                  <br> <br>
                  <strong>Dhruv Shah</strong>,
                  Benjamin Eysenbach,
                  Nicholas Rhinehart,
                  Sergey Levine
                  <p> <i> Conference on Robot Learning (CoRL), 2021 </i> &nbsp; <b style="color:#FDB515">(Oral
                      Presentation)</b>
                    <br> <i> Workshop on Never-Ending Reinforcement Learning at ICLR 2021 </i> &nbsp; <b
                      style="color:#FDB515">(Oral Presentation)</b>
                  </p>
                  <p style="font-size: 13px"> <a href="https://bair.berkeley.edu/blog/2021/11/03/recon/"> Blog Post </a>
                    / <a href="https://arxiv.org/abs/2104.05859">arXiv</a> / <a
                      href="https://www.youtube.com/watch?v=tR9MMvsu2gA">Talk @ CoRL</a> / <a
                      href="https://youtu.be/jF1URT-TSRU">Talk @ ICLR</a> / <a
                      href="https://sites.google.com/view/recon-robot/dataset"> Dataset </a> / <a
                      href="https://www.rsipvision.com/ComputerVisionNews-2021December/4/"> Media Coverage </a> </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='assets/ving_icra21.gif' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="http://sites.google.com/view/ving-robot/">
                    <papertitle>ViNG: Learning Open-World Navigation with Visual Goals</papertitle>
                  </a>
                  <br> <br>
                  <strong>Dhruv Shah</strong>,
                  Benjamin Eysenbach,
                  Gregory Kahn,
                  Nicholas Rhinehart,
                  Sergey Levine
                  <p> <i> International Conference on Robotics and Automation (ICRA), 2021 </i></p>
                  <p style="font-size: 13px"> <a href="https://arxiv.org/abs/2012.09812">arXiv</a> / <a
                      href="https://www.youtube.com/watch?v=57h7D-loifg">Summary Video</a> </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='assets/drawdrone_rss20.gif' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="http://www.roboticsproceedings.org/rss16/p046.html">
                    <papertitle>Aerial Manipulation Using Hybrid Force and Position NMPC Applied to Aerial Writing
                    </papertitle>
                  </a>
                  <br> <br>
                  Dimos Tzoumanikas,
                  Felix Graule,
                  Qingyue Yan,
                  <strong>Dhruv Shah</strong>,
                  Marija Popovic,
                  Stefan Leutenegger
                  <p> <i> Robotics: Science and Systems (RSS), 2020 </i></p>
                  <p style="font-size: 13px"> <a href="https://arxiv.org/abs/2006.02116">arXiv</a> / <a
                      href="https://www.youtube.com/watch?v=slBsougeZhc">Talk @ RSS</a> / <a
                      href="https://www.youtube.com/watch?v=iE--MO0YF0o">Cool Demos</a> </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='assets/ingredients_iclr20.gif' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://sites.google.com/view/realworld-rl/">
                    <papertitle>The Ingredients of Real World Robotic Reinforcement Learning</papertitle>
                  </a>
                  <br> <br>
                  Henry Zhu<sup>†</sup>,
                  Justin Yu<sup>†</sup>,
                  Abhishek Gupta<sup>†</sup>,
                  <strong>Dhruv Shah</strong>,
                  Kristian Hartikainen,
                  Avi Singh,
                  Vikash Kumar,
                  Sergey Levine
                  <p> <i> International Conference on Learning Representations (ICLR), 2020 </i> &nbsp; <b
                      style="color:#FDB515">(Spotlight Presentation)</b>
                  <p style="font-size: 13px"> <a href="https://bair.berkeley.edu/blog/2020/04/27/ingredients/"> Blog
                      Post </a> / <a href="https://arxiv.org/abs/2004.12570">arXiv</a> / <a
                      href="https://www.youtube.com/watch?v=LR6OyQdhEdQ&feature=emb_title">Talk</a> / <a
                      href="https://iclr.cc/virtual_2020/poster_rJe2syrtvS.html">Virtual Presentation</a> </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='assets/swarm_ral19.png' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/document/8613876">
                    <papertitle>Swarm Aggregation Without Communication and Global Positioning</papertitle>
                  </a>
                  <br> <br>
                  <strong>Dhruv Shah</strong>,
                  Leena Vachhani
                  <p> <i> Robotics and Automation Letters (RA-L), 2019 </i>
                    <br> <i>International Conference on Robotics and Automation (ICRA), 2019</i>
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <!-- <img src='assets/projection_icip19.png' width=100%> -->
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="">
                    <papertitle>Projection Design for Compressive Source Separation using Mean Errors and
                      Cross-Validation</papertitle>
                  </a>
                  <br> <br>
                  <strong>Dhruv Shah</strong>,
                  Ajit Rajwade
                  <br> <i> International Conference on Image Processing (ICIP), 2019 </i>
                  <br>
                  <p></p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='assets/projection_globalsip18.png' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="./constrained-projections/">
                    <papertitle>Designing Constrained Projections for Compressed Sensing: Mean Errors and Anomalies with
                      Coherence</papertitle>
                  </a>
                  <br> <br>
                  <strong>Dhruv Shah</strong><sup>†</sup>,
                  Alankar Kotwal<sup>†</sup>,
                  Ajit Rajwade
                  <br> <i> Global Conference on Signal and Information Processing (GlobalSIP), 2018 </i>
                  <br>
                  <p></p>
                </td>
              </tr>

              <!-- <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Combining 3D Mapping and Radiation Source Localization in Nuclear Sites</papertitle>
              </a>
              <br> <br>
              Weikun Zhen<sup>†</sup>,
              <strong>Dhruv Shah</strong><sup>†</sup>, 
              Michael Lee, 
              Matthew Hanczor, 
              Sebastian Scherer
              <br> <i> Under Review </i>
              <br>
              <p></p>
            </td>
          </tr> -->
            </tbody>
          </table>

          <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Miscellaneous</heading>
            </td>
          </tr>
        </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='assets/pommerman_fila.gif' width=100%>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://cs.berkeley.edu/~shah/proj/pommerman.html">
                <papertitle>Multi-Agent Strategies for Pommerman</papertitle>
              </a>
              <br>
              <br> <i> CS747: Foundation of Intelligent & Learning Agents </i>
              <br>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='assets/dpac_board.png' width=100%>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://cs.berkeley.edu/~shah/proj/dpac.html">
                <papertitle>DPAC: Digitally Programmable Analog Computer</papertitle>
              </a>
              <br>
              <br> <i> EE344: Electronics Design Laboratory </i>
              <br>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='assets/rover_15.jpg' width=100%>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://cs.berkeley.edu/~shah/proj/iitb_mars.html">
                <papertitle>The IITB Mars Rover Project</papertitle>
              </a>
              <br>
              <br>
              <br>
              <p></p>
            </td>
          </tr>
        </tbody></table> -->


          <hr width=80%>
          <div id="credits">
            <a href="http://jeffdonahue.com/"> Of course,</a> <a href="https://jonbarron.info/">I did not design this
              page! </a>
          </div>
          <br> <br>
        </td>
      </tr>
  </table>
</body>

</html>
